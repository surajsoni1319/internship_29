{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da39509e",
   "metadata": {},
   "source": [
    "## Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. \n",
    "\n",
    "You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6a40f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e6cf16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cunnecting the web Driver\n",
    "driver=webdriver.Chrome(r'F:\\chromedriver.exe')\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8335b1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"aaa555e7aaf9494bf3b9d00edade1320\", element=\"ac0298a5-32a2-4977-a396-b672ad959242\")>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find web element for search bar using class name\n",
    "search_job = driver.find_element(By.CLASS_NAME,'suggestor-input')\n",
    "search_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b60db357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write on search bar\n",
    "search_job.send_keys(\"Data Analyst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bcd5d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"aaa555e7aaf9494bf3b9d00edade1320\", element=\"89f05c04-df1f-4d41-9114-985b04df38f5\")>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding web element for search location bar using relative Xpath \n",
    "search_loc = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\")\n",
    "search_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c51fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding web element for job locatin bar\n",
    "search_loc.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24fcc798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"aaa555e7aaf9494bf3b9d00edade1320\", element=\"940610ef-8130-49f4-9831-8a8d8f7c83d4\")>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clicking using web absolute xpath function \n",
    "search_btn=driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[6]\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "857bf316",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b52937",
   "metadata": {},
   "source": [
    "## Extracting Job Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c78515c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having job titles\n",
    "title_tags = driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "len(title_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "101eccbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the job title is inside the web element , we will run a for loop to extract.\n",
    "job_titles = []\n",
    "\n",
    "for i in title_tags:\n",
    "    job_titles.append(i.text)\n",
    "len(job_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa96f31",
   "metadata": {},
   "source": [
    "## Extracting Company Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3433c8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having company\n",
    "company_tags = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "len(company_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e08f9e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the company name is inside the web element , we will run a for loop to extract.\n",
    "company_names = []\n",
    "\n",
    "for i in company_tags:\n",
    "    company_names.append(i.text)\n",
    "len(company_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d01ea2c",
   "metadata": {},
   "source": [
    "## extrating job location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93f3078c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having location\n",
    "loc_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "len(loc_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8fb349df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the job location is inside the web element , we will run a for loop to extract.\n",
    "job_loc = []\n",
    "\n",
    "for i in loc_tags:\n",
    "    job_loc.append(i.text)\n",
    "len(job_loc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899c1eb9",
   "metadata": {},
   "source": [
    "## extracting experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "332d2507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having experience\n",
    "exp_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "len(exp_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "69021414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the experience is inside the web element , we will run a for loop to extract.\n",
    "experience = []\n",
    "\n",
    "for i in exp_tags:\n",
    "    experience.append(i.text)\n",
    "len(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9eaa56aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20, 20, 20)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_titles),len(job_loc),len(company_names),len(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0447a58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Company Names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sr.Business Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru, karnataka\\n(WFH during Co...</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "      <td>Collabera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sr Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "      <td>Thomson Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Analysis Analyst</td>\n",
       "      <td>Bangalore/Bengaluru, Pune, Chennai</td>\n",
       "      <td>7-12 Yrs</td>\n",
       "      <td>Capco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>2-3 Yrs</td>\n",
       "      <td>Thomson Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst/Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>Meesho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Financial Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Nuance India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst II</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "      <td>Cerner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Analyst II</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>Flipkart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Management Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "      <td>Wells Fargo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst - CRM Platform</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "      <td>Artech infosystem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Job Title  \\\n",
       "0          Sr.Business Data Analyst   \n",
       "1                   Sr Data Analyst   \n",
       "2      Senior Data Analysis Analyst   \n",
       "3                      Data Analyst   \n",
       "4  Data Analyst/Senior Data Analyst   \n",
       "5            Financial Data Analyst   \n",
       "6                   Data Analyst II   \n",
       "7            Senior Data Analyst II   \n",
       "8    Senior Data Management Analyst   \n",
       "9       Data Analyst - CRM Platform   \n",
       "\n",
       "                                        Job Location Experience  \\\n",
       "0  Bangalore/Bengaluru, karnataka\\n(WFH during Co...   6-11 Yrs   \n",
       "1                                Bangalore/Bengaluru    5-8 Yrs   \n",
       "2                 Bangalore/Bengaluru, Pune, Chennai   7-12 Yrs   \n",
       "3                                Bangalore/Bengaluru    2-3 Yrs   \n",
       "4                                Bangalore/Bengaluru    3-6 Yrs   \n",
       "5                                Bangalore/Bengaluru    3-8 Yrs   \n",
       "6                                Bangalore/Bengaluru    2-4 Yrs   \n",
       "7                                Bangalore/Bengaluru    3-6 Yrs   \n",
       "8                                Bangalore/Bengaluru    4-7 Yrs   \n",
       "9  Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...    1-6 Yrs   \n",
       "\n",
       "       Company Names  \n",
       "0          Collabera  \n",
       "1    Thomson Reuters  \n",
       "2              Capco  \n",
       "3    Thomson Reuters  \n",
       "4             Meesho  \n",
       "5       Nuance India  \n",
       "6             Cerner  \n",
       "7           Flipkart  \n",
       "8        Wells Fargo  \n",
       "9  Artech infosystem  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs=pd.DataFrame()\n",
    "jobs['Job Title']=job_titles\n",
    "jobs['Job Location']=job_loc\n",
    "jobs['Experience']=experience\n",
    "jobs['Company Names']=company_names\n",
    "jobs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc0b515",
   "metadata": {},
   "source": [
    "## Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "42e75354",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cunnecting the web Driver\n",
    "driver=webdriver.Chrome(r'F:\\chromedriver.exe')\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d60aa634",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"92c72687b3a5017611f922f4acfbf87d\", element=\"ade7a9e1-d004-4a53-94b7-58652d653469\")>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find web element for search bar using class name\n",
    "search_job = driver.find_element(By.CLASS_NAME,'suggestor-input')\n",
    "search_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0c6f4d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write on search bar\n",
    "search_job.send_keys(\"Data Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7a87561f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"92c72687b3a5017611f922f4acfbf87d\", element=\"de0bb2b2-8df8-47b2-87c8-d1d5bc0d3d50\")>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding web element for search location bar using relative Xpath \n",
    "search_loc = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\")\n",
    "search_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7ab715a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding web element for job locatin bar\n",
    "search_loc.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0e268c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"92c72687b3a5017611f922f4acfbf87d\", element=\"95743275-913a-4511-82c8-ea9ae35b0e25\")>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clicking using web absolute xpath function \n",
    "search_btn=driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[6]\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "45d5f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7a0c64",
   "metadata": {},
   "source": [
    "### Extracting Job Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "653c4e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having job titles\n",
    "title_tags = driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "len(title_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "81da245c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the job title is inside the web element , we will run a for loop to extract.\n",
    "job_titles = []\n",
    "\n",
    "for i in title_tags:\n",
    "    job_titles.append(i.text)\n",
    "len(job_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6180307",
   "metadata": {},
   "source": [
    "## extrating job location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "82f5e81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having location\n",
    "loc_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "len(loc_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d23db1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the job location is inside the web element , we will run a for loop to extract.\n",
    "job_loc = []\n",
    "\n",
    "for i in loc_tags:\n",
    "    job_loc.append(i.text)\n",
    "len(job_loc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d5751",
   "metadata": {},
   "source": [
    "## Extracting Company Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b47f81aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having company\n",
    "company_tags = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "len(company_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f3bee3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the company name is inside the web element , we will run a for loop to extract.\n",
    "company_names = []\n",
    "\n",
    "for i in company_tags:\n",
    "    company_names.append(i.text)\n",
    "len(company_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3f1f6eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Company Names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist/ Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Pune, Gurgaon/Gurugram, C...</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Urgent Job Opening For AI Practitioner - Data ...</td>\n",
       "      <td>Bangalore/Bengaluru, Kochi/Cochin, New Delhi, ...</td>\n",
       "      <td>Wipro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dataiku Consultant</td>\n",
       "      <td>Bangalore/Bengaluru, Pune, Chennai</td>\n",
       "      <td>Wipro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist: Advanced Analytics</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>IBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Research Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>IBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Science - Senior Data Scientist - Analytics</td>\n",
       "      <td>Bangalore/Bengaluru, Noida</td>\n",
       "      <td>Paytm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Principal - Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Schneider Electric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Research and Development -AI/ML -(PhD )</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Hyderabad/Secunder...</td>\n",
       "      <td>EXL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Opportunity For Data Scientist - Female Candid...</td>\n",
       "      <td>Bangalore/Bengaluru, Gurgaon/Gurugram, Mumbai ...</td>\n",
       "      <td>PayU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Science Engineer</td>\n",
       "      <td>Bangalore/Bengaluru, Gurgaon/Gurugram, Mumbai ...</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0              Data Scientist/ Senior Data Scientist   \n",
       "1  Urgent Job Opening For AI Practitioner - Data ...   \n",
       "2                                 Dataiku Consultant   \n",
       "3                 Data Scientist: Advanced Analytics   \n",
       "4                                 Research Scientist   \n",
       "5   Data Science - Senior Data Scientist - Analytics   \n",
       "6                         Principal - Data Scientist   \n",
       "7            Research and Development -AI/ML -(PhD )   \n",
       "8  Opportunity For Data Scientist - Female Candid...   \n",
       "9                       Senior Data Science Engineer   \n",
       "\n",
       "                                        Job Location       Company Names  \n",
       "0  Bangalore/Bengaluru, Pune, Gurgaon/Gurugram, C...   Fractal Analytics  \n",
       "1  Bangalore/Bengaluru, Kochi/Cochin, New Delhi, ...               Wipro  \n",
       "2                 Bangalore/Bengaluru, Pune, Chennai               Wipro  \n",
       "3                                Bangalore/Bengaluru                 IBM  \n",
       "4                                Bangalore/Bengaluru                 IBM  \n",
       "5                         Bangalore/Bengaluru, Noida               Paytm  \n",
       "6                                Bangalore/Bengaluru  Schneider Electric  \n",
       "7  Bangalore/Bengaluru, Noida, Hyderabad/Secunder...                 EXL  \n",
       "8  Bangalore/Bengaluru, Gurgaon/Gurugram, Mumbai ...                PayU  \n",
       "9  Bangalore/Bengaluru, Gurgaon/Gurugram, Mumbai ...   Fractal Analytics  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs=pd.DataFrame()\n",
    "jobs['Job Title']=job_titles\n",
    "jobs['Job Location']=job_loc\n",
    "jobs['Company Names']=company_names\n",
    "jobs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161791dd",
   "metadata": {},
   "source": [
    "## Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "1. You have to use the location and salary filter.\n",
    "2. You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "3. You have to scrape the job-title, job-location, company name, experience required.\n",
    "4. The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d83c5a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cunnecting the web Driver\n",
    "driver=webdriver.Chrome(r'F:\\chromedriver.exe')\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4e994b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"69a1bac879d7a69395113f5626d62fe5\", element=\"6ebfe050-6e71-4152-9250-4e8b632d371f\")>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find web element for search bar using class name\n",
    "search_job = driver.find_element(By.CLASS_NAME,'suggestor-input')\n",
    "search_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "40088776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write on search bar\n",
    "search_job.send_keys(\"Data Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "da3692d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"69a1bac879d7a69395113f5626d62fe5\", element=\"2b958bb1-f8b8-4d15-a195-276160ab914f\")>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clicking using web absolute xpath function \n",
    "search_btn=driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[6]\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "943744d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on search botton\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "87ee4353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the salary range \n",
    "salary_check=driver.find_element(By.XPATH,\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[6]/div[2]/div[2]/label/i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2df0748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_check.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6e65f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the Location range \n",
    "location_check=driver.find_element(By.XPATH,\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[6]/div[2]/div[3]/label/i\")\n",
    "location_check.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7413068d",
   "metadata": {},
   "source": [
    "### Extracting Job Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "cfbbb0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having job titles\n",
    "title_tags = driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "len(title_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b018968e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the job title is inside the web element , we will run a for loop to extract.\n",
    "job_titles = []\n",
    "\n",
    "for i in title_tags:\n",
    "    job_titles.append(i.text)\n",
    "len(job_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a612249f",
   "metadata": {},
   "source": [
    "## extrating job location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f643dd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having location\n",
    "loc_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "len(loc_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1191b38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the job location is inside the web element , we will run a for loop to extract.\n",
    "job_loc = []\n",
    "\n",
    "for i in loc_tags:\n",
    "    job_loc.append(i.text)\n",
    "len(job_loc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03e36bd",
   "metadata": {},
   "source": [
    "## Extracting Company Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ba7fcc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having company\n",
    "company_tags = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "len(company_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "230ae63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the company name is inside the web element , we will run a for loop to extract.\n",
    "company_names = []\n",
    "\n",
    "for i in company_tags:\n",
    "    company_names.append(i.text)\n",
    "len(company_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670ee1f2",
   "metadata": {},
   "source": [
    "## extracting experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "0e21f6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract all the web element having experience\n",
    "exp_tags = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "len(exp_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "86e2382a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the experience is inside the web element , we will run a for loop to extract.\n",
    "experience = []\n",
    "\n",
    "for i in exp_tags:\n",
    "    experience.append(i.text)\n",
    "len(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "7e68e6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Company Names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DigitalBCG GAMMA Data Scientist</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Associate - Data Science</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Gurgaon/Gurugr...</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "      <td>Black Turtle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist - Noida/Bangalore</td>\n",
       "      <td>Noida, Bangalore/Bengaluru</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>EXL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist - Machine learning AI</td>\n",
       "      <td>Delhi / NCR, Bangalore/Bengaluru, Mumbai (All ...</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Teq Analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist - MIND Infotech</td>\n",
       "      <td>Noida</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "      <td>MOTHERSONSUMI INFOTECH &amp; DESIGNS LIMITED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist - Engine Algorithm</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "      <td>Primo Hiring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Delhi / NCR, Pune, Bangalore/Bengaluru</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "      <td>Mount Talent Consulting Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Delhi / NCR, Pune, Bangalore/Bengaluru</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "      <td>Mount Talent Consulting Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Delhi / NCR, Bangalore/Bengaluru, Mumbai (All ...</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Sydata Consulting India Pvt Ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist - Internet Jobs - II</td>\n",
       "      <td>Delhi / NCR, Bangalore/Bengaluru, Mumbai (All ...</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>Jobs Territory</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Job Title  \\\n",
       "0       DigitalBCG GAMMA Data Scientist   \n",
       "1       Senior Associate - Data Science   \n",
       "2      Data Scientist - Noida/Bangalore   \n",
       "3  Data Scientist - Machine learning AI   \n",
       "4        Data Scientist - MIND Infotech   \n",
       "5     Data Scientist - Engine Algorithm   \n",
       "6                        Data Scientist   \n",
       "7                        Data Scientist   \n",
       "8                        Data Scientist   \n",
       "9   Data Scientist - Internet Jobs - II   \n",
       "\n",
       "                                        Job Location Experience  \\\n",
       "0                     New Delhi, Bangalore/Bengaluru    2-5 Yrs   \n",
       "1  Mumbai, Hyderabad/Secunderabad, Gurgaon/Gurugr...    4-7 Yrs   \n",
       "2                         Noida, Bangalore/Bengaluru   5-10 Yrs   \n",
       "3  Delhi / NCR, Bangalore/Bengaluru, Mumbai (All ...    3-8 Yrs   \n",
       "4                                              Noida    4-8 Yrs   \n",
       "5  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...    1-3 Yrs   \n",
       "6             Delhi / NCR, Pune, Bangalore/Bengaluru    2-4 Yrs   \n",
       "7             Delhi / NCR, Pune, Bangalore/Bengaluru    2-4 Yrs   \n",
       "8  Delhi / NCR, Bangalore/Bengaluru, Mumbai (All ...    3-8 Yrs   \n",
       "9  Delhi / NCR, Bangalore/Bengaluru, Mumbai (All ...    3-6 Yrs   \n",
       "\n",
       "                              Company Names  \n",
       "0                   Boston Consulting Group  \n",
       "1                              Black Turtle  \n",
       "2                                       EXL  \n",
       "3                             Teq Analytics  \n",
       "4  MOTHERSONSUMI INFOTECH & DESIGNS LIMITED  \n",
       "5                              Primo Hiring  \n",
       "6   Mount Talent Consulting Private Limited  \n",
       "7   Mount Talent Consulting Private Limited  \n",
       "8           Sydata Consulting India Pvt Ltd  \n",
       "9                            Jobs Territory  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs=pd.DataFrame()\n",
    "jobs['Job Title']=job_titles\n",
    "jobs['Job Location']=job_loc\n",
    "jobs['Experience']=experience\n",
    "jobs['Company Names']=company_names\n",
    "jobs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44dab30",
   "metadata": {},
   "source": [
    "## Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "\n",
    "\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and\n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the\n",
    "required data as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "e685bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cunnecting the web Driver\n",
    "driver=webdriver.Chrome(r'F:\\chromedriver.exe')\n",
    "url='https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off'\n",
    "driver.get(url)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "a4df5cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"363e53b0bb9aee43f05b010b1e6b8d28\", element=\"79c1d588-1e69-4757-abb2-ae276bdfb2c0\")>"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find web element for search bar using class name\n",
    "search_bar = driver.find_element(By.CLASS_NAME,'_3704LK')\n",
    "search_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "248dcfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write on search bar\n",
    "search_bar.send_keys(\"sunglasses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "2a7a86a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"363e53b0bb9aee43f05b010b1e6b8d28\", element=\"8106d680-692d-4e82-af42-ce9cfa377a3e\")>"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clicking using web absolute xpath function \n",
    "search_btn=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "f3100e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on search botton\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "15199d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_url=[]\n",
    "\n",
    "url=driver.find_elements(By.XPATH,\"//nav[@class='yFHi8N']//a\")\n",
    "for i in url:\n",
    "    page_url.append(i.get_attribute('href'))\n",
    "page_url=page_url[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "e2b73236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list to scraping data \n",
    "\n",
    "brand=[]\n",
    "Description=[]\n",
    "Price=[]\n",
    "Discount=[] \n",
    "\n",
    "#Brand\n",
    "for i in page_url:\n",
    "    driver.get(i)\n",
    "    titles_brand=driver.find_elements(By.XPATH,\"//div[@class='_2WkVRV']\")\n",
    "    for i in titles_brand:\n",
    "        brand.append(i.text)\n",
    "        \n",
    "#Prduct Discription        \n",
    "    titles_Description=driver.find_elements(By.XPATH,\"//a[@class='IRpwTa']\")\n",
    "    for i in titles_Description:\n",
    "        Description.append(i.text)\n",
    "\n",
    "#Price\n",
    "    titles_Price=driver.find_elements(By.XPATH,\"//div[@class='_30jeq3']\")\n",
    "    for i in titles_Price:\n",
    "        Price.append(i.text)\n",
    "        \n",
    "#Discount    \n",
    "    titles_Discount=driver.find_elements(By.XPATH,\"//div[@class='_3Ay6Sb']/span\")\n",
    "    for i in titles_Discount:\n",
    "        Discount.append(i.text.replace(\"% off\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "64432a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DAHAAZIL</td>\n",
       "      <td>UV Protection, Night Vision, Riding Glasses Wa...</td>\n",
       "      <td>₹177</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹639</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Specs</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹264</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Rectangul...</td>\n",
       "      <td>₹749</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹315</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Retro Square Sunglasse...</td>\n",
       "      <td>₹474</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>Polarized, UV Protection Round Sunglasses (51)</td>\n",
       "      <td>₹1,099</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Urbanic</td>\n",
       "      <td>Others Retro Square Sunglasses (Free Size)</td>\n",
       "      <td>₹399</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Cat-eye S...</td>\n",
       "      <td>₹799</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>elegante</td>\n",
       "      <td>Polarized, Riding Glasses, Night Vision Sports...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand                                Product Description   Price  \\\n",
       "0         DAHAAZIL  UV Protection, Night Vision, Riding Glasses Wa...    ₹177   \n",
       "1         Fastrack   UV Protection Rectangular Sunglasses (Free Size)    ₹639   \n",
       "2        New Specs   UV Protection Rectangular Sunglasses (Free Size)    ₹264   \n",
       "3    VINCENT CHASE  by Lenskart Polarized, UV Protection Rectangul...    ₹749   \n",
       "4        Elligator                UV Protection Round Sunglasses (54)    ₹315   \n",
       "..             ...                                                ...     ...   \n",
       "95  ROZZETTA CRAFT  UV Protection, Gradient Retro Square Sunglasse...    ₹474   \n",
       "96   VINCENT CHASE     Polarized, UV Protection Round Sunglasses (51)  ₹1,099   \n",
       "97         Urbanic         Others Retro Square Sunglasses (Free Size)    ₹399   \n",
       "98   VINCENT CHASE  by Lenskart Polarized, UV Protection Cat-eye S...    ₹799   \n",
       "99        elegante  Polarized, Riding Glasses, Night Vision Sports...    ₹499   \n",
       "\n",
       "   Discount %  \n",
       "0          82  \n",
       "1          20  \n",
       "2          89  \n",
       "3          62  \n",
       "4          87  \n",
       "..        ...  \n",
       "95         76  \n",
       "96         45  \n",
       "97         49  \n",
       "98         60  \n",
       "99         78  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for the first 100 sunglasses \n",
    "\n",
    "flipkart=pd.DataFrame({})\n",
    "flipkart['Brand']=brand[0:100]\n",
    "flipkart['Product Description']=Description[0:100]\n",
    "flipkart['Price']=Price[0:100]\n",
    "flipkart['Discount %']=Discount[0:100]\n",
    "\n",
    "flipkart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104534a",
   "metadata": {},
   "source": [
    "## Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.flipkart.com/\n",
    "2. Enter “iphone 11” in “Search” field .\n",
    "3. Then click the search button.\n",
    "You will reach to the below shown webpage .\n",
    "As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews.\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5da0166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the webpage through our web driver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "b=driver.find_element(By.XPATH,'/html/body/div[2]/div/div/button')\n",
    "b.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea75b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on search button\n",
    "search_button=driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4436ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-white-64-gb/p/itmfc6a7091eb20b?pid=MOBFWQ6BVWVEH3XE&lid=LSTMOBFWQ6BVWVEH3XEB1SFMZ&marketplace=FLIPKART&q=iphone+11&store=tyy%2F4io&srno=s_1_1&otracker=search&otracker1=search&fm=organic&iid=ffcd465c-1c35-4a3b-8c2b-8d0bc138972c.MOBFWQ6BVWVEH3XE.SEARCH&ppt=hp&ppn=homepage&ssid=x6aouugn8w0000001654762626827&qH=f6cdfdaa9f3c23f3\")\n",
    "phone=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[9]/div[6]/div/a/div')\n",
    "phone.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c8b7b4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating empty list to scraping data \n",
    "rating=[]\n",
    "review_sum=[]\n",
    "review_dis=[]\n",
    "j=1\n",
    "# extracting all the tags\n",
    "def i_scrap():\n",
    "    rate=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    for i in rate:\n",
    "        rating.append(i.text)\n",
    "    review=driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "    for i in review:\n",
    "        review_sum.append(i.text)\n",
    "    rdis=driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]')\n",
    "    for i in rdis:\n",
    "        review_dis.append(i.text)\n",
    "  #scraping i\n",
    "next_b=driver.find_elements(By.CLASS_NAME,\"ge-49M\")\n",
    "for b in next_b:\n",
    "    i_scrap()\n",
    "len(rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "496a6d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>Discription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>What a camera .....just awesome ..you can feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>This is my first iOS phone. I am very happy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.\\n\\nI’m am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>i11 is worthy to buy, too much happy with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>4</td>\n",
       "      <td>Good choice</td>\n",
       "      <td>So far it’s been an AMAZING experience coming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>iphone 11 is a very good phone to buy only if ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating               Review  \\\n",
       "0       5       Simply awesome   \n",
       "1       5     Perfect product!   \n",
       "2       5  Best in the market!   \n",
       "3       5   Highly recommended   \n",
       "4       5    Worth every penny   \n",
       "..    ...                  ...   \n",
       "95      5            Fabulous!   \n",
       "96      5        Great product   \n",
       "97      5    Worth every penny   \n",
       "98      4          Good choice   \n",
       "99      5   Highly recommended   \n",
       "\n",
       "                                          Discription  \n",
       "0   Really satisfied with the Product I received.....  \n",
       "1   Amazing phone with great cameras and better ba...  \n",
       "2   Great iPhone very snappy experience as apple k...  \n",
       "3   What a camera .....just awesome ..you can feel...  \n",
       "4   Previously I was using one plus 3t it was a gr...  \n",
       "..                                                ...  \n",
       "95  This is my first iOS phone. I am very happy wi...  \n",
       "96  Amazing Powerful and Durable Gadget.\\n\\nI’m am...  \n",
       "97  i11 is worthy to buy, too much happy with the ...  \n",
       "98  So far it’s been an AMAZING experience coming ...  \n",
       "99  iphone 11 is a very good phone to buy only if ...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for the first 100 reviews data from flipkart.com for iphone11 phone:\n",
    "iphone_11=pd.DataFrame({'Rating':rating,'Review':review_sum,'Discription':review_dis})\n",
    "iphone_11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d85a0b1",
   "metadata": {},
   "source": [
    "## Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "d3b06e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "6cd2c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter “sneakers” in “search for products, brands and more” field.\n",
    "search_product = driver.find_element(By.XPATH,\"//div[@class='_3OO5Xc']/input\")\n",
    "search_product.send_keys(\"sneakers\")\n",
    "# click the search button\n",
    "search_btn = driver.find_element(By.XPATH,\"//button[@class='L0Z3Pu']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "68afaee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying the url of the webpage to be scraped\n",
    "url = \"https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "\n",
    "# open the webpage through our web driver\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "2701e654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating first 3 website pages to scrape the data\n",
    "\n",
    "page_url=[]\n",
    "\n",
    "url=driver.find_elements(By.XPATH,\"//nav[@class='yFHi8N']//a\")\n",
    "for i in url:\n",
    "    page_url.append(i.get_attribute('href'))\n",
    "page_url=page_url[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "0d4c72b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list to scraping data into them\n",
    "\n",
    "brand=[]\n",
    "Description=[]\n",
    "Price=[]\n",
    "Discount=[] \n",
    "\n",
    "# extracting all the tags\n",
    "\n",
    "for i in page_url:\n",
    "    driver.get(i)\n",
    "    titles_brand=driver.find_elements(By.XPATH,\"//div[@class='_2WkVRV']\")\n",
    "    for i in titles_brand:\n",
    "        brand.append(i.text)\n",
    "        \n",
    "    titles_Description=driver.find_elements(By.XPATH,\"//a[@class='IRpwTa']\")\n",
    "    for i in titles_Description:\n",
    "        Description.append(i.text)\n",
    "        \n",
    "    titles_Price=driver.find_elements(By.XPATH,\"//div[@class='_30jeq3']\")\n",
    "    for i in titles_Price:\n",
    "        Price.append(i.text.replace(\"₹\",\"\"))\n",
    "        \n",
    "    titles_Discount=driver.find_elements(By.XPATH,\"//div[@class='_3Ay6Sb']/span\")\n",
    "    for i in titles_Discount:\n",
    "        Discount.append(i.text.replace(\"% off\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "8ac315ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 sneakers listings on flipkart.com:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price₹</th>\n",
       "      <th>Discount %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LE GREEM</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>499</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LOUIS PHILIPPE</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>2,399</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WOODLAND</td>\n",
       "      <td>Modern Trendy Sneakers Shoes Sneakers For Men</td>\n",
       "      <td>2,096</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Modern Trendy Sneakers Shoes Sneakers For Men</td>\n",
       "      <td>269</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>299</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>HOTSTYLE</td>\n",
       "      <td>Men's Sneakers Fashion Lightweight Running Sho...</td>\n",
       "      <td>271</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>LE GREEM</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>499</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>KNIGHT WALKERS</td>\n",
       "      <td>Stylish White Casual Walking Sneakers Shoes Fo...</td>\n",
       "      <td>648</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Layasa</td>\n",
       "      <td>Canvas Casual Partywear Outdoor Sneakers Shoes...</td>\n",
       "      <td>499</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ASIAN</td>\n",
       "      <td>X-Ray 2 Square Sneakers For Men</td>\n",
       "      <td>498</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand                                Product Description Price₹  \\\n",
       "0         LE GREEM                                   Sneakers For Men    499   \n",
       "1   LOUIS PHILIPPE                                   Sneakers For Men  2,399   \n",
       "2         WOODLAND      Modern Trendy Sneakers Shoes Sneakers For Men  2,096   \n",
       "3           BRUTON      Modern Trendy Sneakers Shoes Sneakers For Men    269   \n",
       "4           BRUTON                                   Sneakers For Men    299   \n",
       "..             ...                                                ...    ...   \n",
       "95        HOTSTYLE  Men's Sneakers Fashion Lightweight Running Sho...    271   \n",
       "96        LE GREEM                                   Sneakers For Men    499   \n",
       "97  KNIGHT WALKERS  Stylish White Casual Walking Sneakers Shoes Fo...    648   \n",
       "98          Layasa  Canvas Casual Partywear Outdoor Sneakers Shoes...    499   \n",
       "99           ASIAN                    X-Ray 2 Square Sneakers For Men    498   \n",
       "\n",
       "   Discount %  \n",
       "0          50  \n",
       "1          40  \n",
       "2          30  \n",
       "3          79  \n",
       "4          76  \n",
       "..        ...  \n",
       "95         45  \n",
       "96         50  \n",
       "97         56  \n",
       "98         50  \n",
       "99         37  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for the first 100 sneakers:\n",
    "\n",
    "flipkart=pd.DataFrame({})\n",
    "flipkart['Brand']=brand[0:100]\n",
    "flipkart['Product Description']=Description[0:100]\n",
    "flipkart['Price₹']=Price[0:100]\n",
    "flipkart['Discount %']=Discount[0:100]\n",
    "\n",
    "print(\"Top 100 sneakers listings on flipkart.com:\\n\")\n",
    "flipkart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0845df",
   "metadata": {},
   "source": [
    "## Q7: Go to the link - https://www.myntra.com/shoes\n",
    "Set second Price filter and Color filter to “Black”, as shown in the below image.\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe\n",
    "description, price of the shoe as shown in the below image.\n",
    "\n",
    "Note: Applying the filter and scraping the data, everything should be done through code only and there\n",
    "should not be any manual step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "358d6582",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.myntra.com/shoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab48de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the Price filter to “Rs. 7149 to Rs. 14099” by checking the respective boxe\n",
    "\n",
    "price = driver.find_elements(By.XPATH,\"//ul[@class ='price-list']//label[@class ='common-customCheckbox vertical-filters-label']/div[@class = 'common-checkboxIndicator']\") \n",
    "price[1].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fde8719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the Color filter to “Black”  by checking the box\n",
    "\n",
    "color = driver.find_elements(By.XPATH,\"//li[@class ='colour-listItem']//label[@class ='common-customCheckbox']/div[@class = 'common-checkboxIndicator']\") \n",
    "color[0].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47afc9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying the url of the webpage to be scraped\n",
    "url = \"https://www.myntra.com/shoes?f=Color%3ABlack_36454f&rf=Price%3A7149.0_14099.0_7149.0%20TO%2014099.0\"\n",
    "\n",
    "# open the webpage through our web driver\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd6adc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating first 3 website pages to scrape the data\n",
    "\n",
    "page_url=[]\n",
    "\n",
    "url=driver.find_elements(By.XPATH,\"//li[@class='pagination-number']//a\")\n",
    "for i in url:\n",
    "    page_url.append(i.get_attribute('href'))\n",
    "page_url=page_url[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb09d192",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating empty list to scraping data into them\n",
    "\n",
    "brand=[]\n",
    "Description=[]\n",
    "Price=[]\n",
    "\n",
    "# extracting all the tags\n",
    "\n",
    "for i in page_url:\n",
    "    titles_brand=driver.find_elements(By.XPATH,\"//h3[@class='product-brand']\")\n",
    "    for i in titles_brand:\n",
    "        brand.append(i.text)\n",
    "    \n",
    "    titles_Description=driver.find_elements(By.XPATH,\"//h4[@class='product-product']\")\n",
    "    for i in titles_Description:\n",
    "        Description.append(i.text)\n",
    "        \n",
    "    titles_Price=driver.find_elements(By.XPATH,\"//div[@class='product-price']\")\n",
    "    for i in titles_Price:\n",
    "        Price.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dc2f656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 sneakers listings on myntra.com:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Shoe Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASICS</td>\n",
       "      <td>Men GEL-Quantum 360 6 Sports</td>\n",
       "      <td>Rs. 13999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hush Puppies</td>\n",
       "      <td>Men Solid Leather Formal Slip-Ons</td>\n",
       "      <td>Rs. 8499Rs. 9999(15% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Women Ballerinas Flats</td>\n",
       "      <td>Rs. 9891Rs. 10990(10% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fitflop</td>\n",
       "      <td>Flatform Sandals with Buckles</td>\n",
       "      <td>Rs. 7499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Perforations Sneakers</td>\n",
       "      <td>Rs. 12591Rs. 13990(10% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>fitflop</td>\n",
       "      <td>Embellished Leather Flatform Sandals</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>fitflop</td>\n",
       "      <td>Embellished PU Comfort Pumps</td>\n",
       "      <td>Rs. 7499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>fitflop</td>\n",
       "      <td>Embellished Leather Wedge Sandals</td>\n",
       "      <td>Rs. 7499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Women Leather Slip-On Sneakers</td>\n",
       "      <td>Rs. 7693Rs. 10990(30% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Columbia</td>\n",
       "      <td>Women REDMOND V2 TrekkingShoe</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Brand                      Shoe Description  \\\n",
       "0          ASICS          Men GEL-Quantum 360 6 Sports   \n",
       "1   Hush Puppies     Men Solid Leather Formal Slip-Ons   \n",
       "2           Geox                Women Ballerinas Flats   \n",
       "3        fitflop         Flatform Sandals with Buckles   \n",
       "4           Geox             Men Perforations Sneakers   \n",
       "..           ...                                   ...   \n",
       "95       fitflop  Embellished Leather Flatform Sandals   \n",
       "96       fitflop          Embellished PU Comfort Pumps   \n",
       "97       fitflop     Embellished Leather Wedge Sandals   \n",
       "98          Geox        Women Leather Slip-On Sneakers   \n",
       "99      Columbia         Women REDMOND V2 TrekkingShoe   \n",
       "\n",
       "                          Price  \n",
       "0                     Rs. 13999  \n",
       "1     Rs. 8499Rs. 9999(15% OFF)  \n",
       "2    Rs. 9891Rs. 10990(10% OFF)  \n",
       "3                      Rs. 7499  \n",
       "4   Rs. 12591Rs. 13990(10% OFF)  \n",
       "..                          ...  \n",
       "95                     Rs. 7999  \n",
       "96                     Rs. 7499  \n",
       "97                     Rs. 7499  \n",
       "98   Rs. 7693Rs. 10990(30% OFF)  \n",
       "99                     Rs. 7999  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for the first 100 sneakers listings on myntra.com:\n",
    "\n",
    "myntra =pd.DataFrame({})\n",
    "myntra['Brand']=brand[0:100]\n",
    "myntra['Shoe Description']=Description[0:100]\n",
    "myntra['Price']=Price[0:100]\n",
    "\n",
    "print(\"Top 100 sneakers listings on myntra.com:\\n\")\n",
    "myntra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cebf1858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#close the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f569e52",
   "metadata": {},
   "source": [
    "## Q8: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cb19dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.amazon.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9299311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter “laptop” in the search field.\n",
    "search_product = driver.find_element(By.XPATH,\"//div[@class='nav-search-field ']/input\")\n",
    "search_product.send_keys(\"laptop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22b94e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click the search button\n",
    "search_btn = driver.find_element(By.XPATH,\"//div[@class='nav-search-submit nav-sprite']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1d555d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying CPU Type filter to “Intel Core i7” filter by checking the respective box\n",
    "i7_clk=driver.find_elements(By.XPATH,\"//span[@class='a-size-base a-color-base']\")\n",
    "for i in i7_clk:\n",
    "    if i.text == 'Intel Core i7':   \n",
    "        i.click() # Checking the checkbox to apply filter on the laptop\n",
    "        break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "664ec4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list to scraping data into them\n",
    "\n",
    "Title=[]\n",
    "Rating=[]\n",
    "Price=[]\n",
    "\n",
    "# extracting all the tags\n",
    "\n",
    "titles=driver.find_elements(By.XPATH,\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']\")\n",
    "for i in titles:\n",
    "    Title.append(i.text)\n",
    "    \n",
    "titles_rat=driver.find_elements(By.XPATH,\"//div[@class='a-row a-size-small']/span\")\n",
    "for i in titles_rat:\n",
    "    Rating.append(i.get_attribute(\"aria-label\"))\n",
    "        \n",
    "titles_Price=driver.find_elements(By.XPATH,\"//span[@class='a-price']\")\n",
    "for i in titles_Price:\n",
    "    Price.append(i.text[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dcbc73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extratcing ratings from the unorganised rating tag\n",
    "\n",
    "rating=[]\n",
    "for i in range(0,len(Rating)):\n",
    "    if i == 0 or  i/2 == i//2:\n",
    "        rating.append(Rating[i][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97f8247a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data for the first 10 laptops data on amazon.com:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>89,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hp Pavilion 15 12Th Gen Intel Core I7 16Gb Sdr...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>87,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....</td>\n",
       "      <td>4.3</td>\n",
       "      <td>86,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Infinix INBook X1 Pro Core i7 10th Gen - (16 G...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>53,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hp Pavilion X360 11Th Gen Intel Core I7 14 Inc...</td>\n",
       "      <td>3.7</td>\n",
       "      <td>82,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HP Envy 11th Gen Intel Evo Core i7 14 inch(35....</td>\n",
       "      <td>4.2</td>\n",
       "      <td>99,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ASUS Vivobook 15, 15.6-inch (39.62 cms) FHD, I...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>57,890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LG Gram 16 Intel Evo 11th Gen i7 Thin &amp; Light ...</td>\n",
       "      <td>3.7</td>\n",
       "      <td>85,499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HP Pavilion 14 12th Gen Intel Core i7 16GB SDR...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>86,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lenovo ThinkBook 13s Intel 11th Gen Core i7 13...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>89,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title Rating   Price\n",
       "0  ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...    4.6  89,990\n",
       "1  Hp Pavilion 15 12Th Gen Intel Core I7 16Gb Sdr...    4.0  87,900\n",
       "2  Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....    4.3  86,990\n",
       "3  Infinix INBook X1 Pro Core i7 10th Gen - (16 G...    4.2  53,999\n",
       "4  Hp Pavilion X360 11Th Gen Intel Core I7 14 Inc...    3.7  82,990\n",
       "5  HP Envy 11th Gen Intel Evo Core i7 14 inch(35....    4.2  99,400\n",
       "6  ASUS Vivobook 15, 15.6-inch (39.62 cms) FHD, I...    4.3  57,890\n",
       "7  LG Gram 16 Intel Evo 11th Gen i7 Thin & Light ...    3.7  85,499\n",
       "8  HP Pavilion 14 12th Gen Intel Core i7 16GB SDR...    3.8  86,990\n",
       "9  Lenovo ThinkBook 13s Intel 11th Gen Core i7 13...    4.0  89,990"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for the first 10 laptops data on amazon.com:\n",
    "\n",
    "amazon =pd.DataFrame({})\n",
    "amazon['Title']=Title[0:10]\n",
    "amazon['Rating']=rating[0:10]\n",
    "amazon['Price']=Price[0:10]\n",
    "\n",
    "print(\"The data for the first 10 laptops data on amazon.com:\\n\")\n",
    "amazon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834db99b",
   "metadata": {},
   "source": [
    "## Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida\n",
    "location. You have to scrape company name, No. of days ago when job was posted, Rating of the company.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image\n",
    "3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter\n",
    "“Data Scientist” and click on search button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f930de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "driver.get(\" https://www.ambitionbox.com//\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "591dbb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click the \"Jobs\" webpage\n",
    "cl_job = driver.find_element(By.XPATH,\"//a[@class='link jobs']\")\n",
    "cl_job.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "506d3361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In place of “Search by Designations, Companies, Skills” enter “Data Scientist” \n",
    "\n",
    "search_title = driver.find_element(By.XPATH,\"//span[@class='twitter-typeahead']/input\")\n",
    "search_title.send_keys(\"Data Scientist\")\n",
    "\n",
    "#click on search button.\n",
    "search_btn = driver.find_element(By.XPATH,\"//button[@class='ab_btn search-btn round']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a557165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_loc = driver.find_element(By.XPATH,\"//div[@class='show-flex']/div[2]\") # Location dropdown\n",
    "cl_loc.click()    \n",
    "driver.find_element(By.XPATH,'/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[2]/input').send_keys('Noida')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "844f2ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH,'/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[3]/div[1]/div[1]/div/label').click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60ada014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping company name, No. of days ago when job was posted, Rating of the company\n",
    "\n",
    "#creating empty list\n",
    "company_name = []\n",
    "job_posted = []\n",
    "rating = []\n",
    "\n",
    "#Scraing all tags\n",
    "\n",
    "company=driver.find_elements(By.XPATH,\"//div[@class='company-info']\")\n",
    "for i in company:\n",
    "    company_name.append(i.text.split(\"\\n\")[0])\n",
    "    rating.append(i.text.split(\"\\n\")[1])\n",
    "    \n",
    "posted=driver.find_elements(By.XPATH,\"//span[@class='body-small-l']\")\n",
    "for i in posted:\n",
    "    job_posted.append(i.text)\n",
    "    \n",
    "# extratcing job posted days from the unorganised job posted tag\n",
    "\n",
    "posted=[]\n",
    "for i in range(0,len(job_posted)):\n",
    "    if i == 0 or  i/2 == i//2:\n",
    "        posted.append(job_posted[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ef03293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data for the first 10 jobs results in ambitionbox.com :\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company_name</th>\n",
       "      <th>Job_Posted</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Optum Global Solutions (India) Private Limited</td>\n",
       "      <td>8d ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>15d ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dew Solutions Pvt. Ltd.</td>\n",
       "      <td>6d ago</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>InfoEdge India Ltd.</td>\n",
       "      <td>13d ago</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Info Edge India Limited</td>\n",
       "      <td>14d ago</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Info Edge India Limited</td>\n",
       "      <td>14d ago</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Latent bridge</td>\n",
       "      <td>14d ago</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Careerera</td>\n",
       "      <td>1d ago</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Careernet Consulting</td>\n",
       "      <td>28d ago</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Acidaes Solutions Pvt. Ltd.</td>\n",
       "      <td>14d ago</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Company_name Job_Posted Rating\n",
       "0  Optum Global Solutions (India) Private Limited     8d ago    4.1\n",
       "1                   GENPACT India Private Limited    15d ago    4.0\n",
       "2                         Dew Solutions Pvt. Ltd.     6d ago    4.3\n",
       "3                             InfoEdge India Ltd.    13d ago    3.9\n",
       "4                         Info Edge India Limited    14d ago    3.9\n",
       "5                         Info Edge India Limited    14d ago    3.9\n",
       "6                                   Latent bridge    14d ago    4.5\n",
       "7                                       Careerera     1d ago    3.8\n",
       "8                            Careernet Consulting    28d ago    3.9\n",
       "9                     Acidaes Solutions Pvt. Ltd.    14d ago    3.8"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating Dataframe \n",
    "\n",
    "ambition = pd.DataFrame({})\n",
    "ambition[\"Company_name\"] = company_name[1:]\n",
    "ambition[\"Job_Posted\"] = posted\n",
    "ambition[\"Rating\"] = rating[1:]\n",
    "\n",
    "print(\"The data for the first 10 jobs results in ambitionbox.com :\\n \")\n",
    "ambition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae8650",
   "metadata": {},
   "source": [
    "## Q10: Write a python program to scrape the salary data for Data Scientist designation.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary.\n",
    "The above task will be, done as shown in the below steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the salaries option as shown in the image.\n",
    "3. After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and\n",
    "then click on “Data Scientist”.\n",
    "You have to scrape the data ticked in the above image.\n",
    "4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average\n",
    "salary, minimum salary, maximum salary, experience required.\n",
    "5. Store the data in a dataframe.\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ff35b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "driver.get(\" https://www.ambitionbox.com//\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ddb128d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on Salary webpage\n",
    "driver.find_element(By.XPATH,\"//a[@class='link salaries']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad1fc456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In place of “Search Job Profile” enters “Data Scientist” and then click on “Data Scientist”.\n",
    "search_job = driver.find_element(By.ID,\"jobProfileSearchbox\")\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "clk=driver.find_elements(By.XPATH,\"//div[@class='suggestion_wrap tt-suggestion tt-selectable']\")\n",
    "for i in clk:\n",
    "    if i.text == 'Data Scientist':   \n",
    "        i.click() # click on “Data Scientist”\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73abb95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape Company name, Number of salaries, Average salary, Min salary, Max Salary, experience required.\n",
    "\n",
    "company_name = []\n",
    "salary_record = []\n",
    "average_salary = []\n",
    "min_salary = []\n",
    "max_salary = []\n",
    "exp = []\n",
    "\n",
    "#Scraing all tags\n",
    "\n",
    "company=driver.find_elements(By.XPATH,\"//div[@class='company-info']\")\n",
    "for i in company:\n",
    "    \n",
    "    #company name\n",
    "    company_name.append(i.text.split(\"\\n\")[0]) \n",
    "    \n",
    "     #Total Salary Record\n",
    "    salary_record.append(i.text.replace(\"based on\", \" \").replace(\"salaries\",\"\").split(\"\\n\")[1:2])\n",
    "    \n",
    "    # Experience required\n",
    "    exp.append(i.text.replace(\"yrs exp\",\"\").replace(\"yr exp\",\"\").split(\"\\n\")[-1])                \n",
    "    \n",
    "avg_salary=driver.find_elements(By.XPATH,\"//div[@class='average-indicator-wrapper']\")\n",
    "for i in avg_salary:\n",
    "    average_salary.append(i.text.replace(\"₹\",\"\").replace(\"L\",\"\"))            # average salary\n",
    "\n",
    "salary=driver.find_elements(By.XPATH,\"//div[@class='salary-values']\")\n",
    "for i in salary:\n",
    "    min_salary.append(i.text.replace(\"₹\",\"\").replace(\"L\",\"\").split(\"\\n\")[0])  # minimum salary\n",
    "    max_salary.append(i.text.replace(\"₹\",\"\").replace(\"L\",\"\").split(\"\\n\")[1])  # maximum salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "034c41e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " first 10 Companies results in ambitionbox.com :\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Total_Salary_Record</th>\n",
       "      <th>Experience_Required</th>\n",
       "      <th>Average_Salary_L</th>\n",
       "      <th>Minimum_Salary_L</th>\n",
       "      <th>Maximum_Salary_L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google</td>\n",
       "      <td>[Software Engineer Salary]</td>\n",
       "      <td>1-3 erience (based on 35 salaries)</td>\n",
       "      <td>31.3</td>\n",
       "      <td>11.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Microsoft Corporation</td>\n",
       "      <td>[Software Engineer Salary]</td>\n",
       "      <td>1-4 erience (based on 274 salaries)</td>\n",
       "      <td>24.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Goldman Sachs</td>\n",
       "      <td>[Software Engineer Salary]</td>\n",
       "      <td>2 erience (based on 18 salaries)</td>\n",
       "      <td>23.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tekion</td>\n",
       "      <td>[Software Engineer Salary]</td>\n",
       "      <td>2-4 erience (based on 35 salaries)</td>\n",
       "      <td>21.2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>[Software Engineer Salary]</td>\n",
       "      <td>1-4 erience (based on 122 salaries)</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Servicenow Software Development India</td>\n",
       "      <td>[Software Engineer Salary]</td>\n",
       "      <td>2-4 erience (based on 60 salaries)</td>\n",
       "      <td>20.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Flipkart</td>\n",
       "      <td>[Software Engineer Salary]</td>\n",
       "      <td>1-4 erience (based on 64 salaries)</td>\n",
       "      <td>20.4</td>\n",
       "      <td>7.5</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>[Software Engineer Salary]</td>\n",
       "      <td>1-4 erience (based on 87 salaries)</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>32.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PayPal</td>\n",
       "      <td>[Software Engineer Salary]</td>\n",
       "      <td>1-2 erience (based on 29 salaries)</td>\n",
       "      <td>19.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Myntra</td>\n",
       "      <td>[Software Engineer Salary]</td>\n",
       "      <td>1 erience (based on 10 salaries)</td>\n",
       "      <td>19.5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Company_Name         Total_Salary_Record  \\\n",
       "0                                 Google  [Software Engineer Salary]   \n",
       "1                  Microsoft Corporation  [Software Engineer Salary]   \n",
       "2                          Goldman Sachs  [Software Engineer Salary]   \n",
       "3                                 Tekion  [Software Engineer Salary]   \n",
       "4                                 Amazon  [Software Engineer Salary]   \n",
       "5  Servicenow Software Development India  [Software Engineer Salary]   \n",
       "6                               Flipkart  [Software Engineer Salary]   \n",
       "7                                Walmart  [Software Engineer Salary]   \n",
       "8                                 PayPal  [Software Engineer Salary]   \n",
       "9                                 Myntra  [Software Engineer Salary]   \n",
       "\n",
       "                   Experience_Required Average_Salary_L Minimum_Salary_L  \\\n",
       "0   1-3 erience (based on 35 salaries)             31.3             11.0   \n",
       "1  1-4 erience (based on 274 salaries)             24.0             13.0   \n",
       "2     2 erience (based on 18 salaries)             23.0             12.0   \n",
       "3   2-4 erience (based on 35 salaries)             21.2             12.0   \n",
       "4  1-4 erience (based on 122 salaries)             21.0              8.0   \n",
       "5   2-4 erience (based on 60 salaries)             20.5             13.0   \n",
       "6   1-4 erience (based on 64 salaries)             20.4              7.5   \n",
       "7   1-4 erience (based on 87 salaries)             20.0             11.4   \n",
       "8   1-2 erience (based on 29 salaries)             19.9             11.0   \n",
       "9     1 erience (based on 10 salaries)             19.5             14.0   \n",
       "\n",
       "  Maximum_Salary_L  \n",
       "0             65.0  \n",
       "1             50.0  \n",
       "2             34.0  \n",
       "3             33.0  \n",
       "4             45.0  \n",
       "5             28.0  \n",
       "6             31.0  \n",
       "7             32.5  \n",
       "8             31.0  \n",
       "9             27.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating Dataframe for the company name, total salary record, average salary, minimum salary, maximum salary, experience required.\n",
    "\n",
    "ambition =pd.DataFrame({})\n",
    "ambition[\"Company_Name\"] = company_name[:10]\n",
    "ambition[\"Total_Salary_Record\"] = salary_record[:10]\n",
    "ambition[\"Experience_Required\"] = exp[:10]\n",
    "ambition[\"Average_Salary_L\"] = average_salary[:10]\n",
    "ambition[\"Minimum_Salary_L\"] = min_salary\n",
    "ambition[\"Maximum_Salary_L\"] = max_salary\n",
    "\n",
    "print(\" first 10 Companies results in ambitionbox.com :\\n \")\n",
    "ambition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
